\chapter{Literature Review}

This literature review is divided in two main sections. The first section is dedicated to the analysis of the constructs that have been investigated in assessing language proficiency, and among those, which ones could serve in an adaptive learning system, while the second one dwell on the statistical ways to score and analyse a given construct. The guiding criterion throughout this chapter will be the simplicity of the solutions proposed, because, in any case, it is always easier to fix a simple system's shortcomings than those of a complex system.

\section{The Proficiency Constructs and Where to Find Them}
    \abbrv{LS}{Learning Science}
    \abbrv{SLA}{Second Language Acquisition}
The introduction exposed how the definition of the instrumental goals that a recommender system has to optimize belongs in the domain of expertise relating to the final goal of the system, rather than in the technology itself. Language testing has traditionally been the matter of Second Language Acquisition (SLA) research, which can be seen as a subdomain of learning science (LS), but this field takes inputs from – and is closely related to – psycholinguistics, applied linguistics, and as we shall see, neuroscience, on which it depends for a general understanding of the processes involved in language use and acquisition. Without pretend to an exhaustive review, this section will attempt to provide a unified overview of language proficiency and the ways to measure it.

This question has been widely studied within various theoretical frameworks and for several practical purposes. Most noticeably, on the mastery of a language can depend the access to citizenship, educational institutions or work positions, which are live-making opportunities that have made its validation a social mobility issue. In this section, we start with the most widely accepted and used way to assess language skills, before moving towards alternative solutions that would fit the needs of a scalable adaptive learning system. Finally, we critically assess these alternatives. The second section is dedicated to finding ways to address the shortcomings of these alternatives.

    \subsection{The Holistic Approach to Testing (CEFR)}
        \abbrv{CEFR}{Common European Framework of Reference for languages}
The complex latent traits like language proficiency can be assessed by two testing paradigms, the first being described as maximalist, comprehensive or holistic, and the second minimalist, proxy-based or reductionist. Commercial and institutional language tests such as the IELTS and the Cambridge English Qualifications for English or the DELF and DALF for French, to only cite these, follow a maximalist approach defined by the Common European Framework of Reference for languages (CEFR) \parencite{europe_common_2020}. This framework not only define the now famous six alphanumeric degrees of language mastery, but also the four usage contexts in which it ought to be measured, two modes of usage, oral and written, for two types of activities, reception and production. It measures the linguistic knowledge (vocabulary, grammar and their constituents) together with the four common language skills that language users engage with: listening, speaking, reading and writing. This framework is considered standard beyond the borders of Europe, but in spite of its strengths, it may not suit all contexts in which languages need to be tested.

The main critique that could be levelled at this testing paradigm is the fact that only ten European languages can brag about having CEFR-compliant tests spanning the six proficiency levels it defines \parencite{noauthor_common_2025, noauthor_cadre_2025}. After twenty-five years of existence, even national languages of leading economies of the EU like Dutch or Czech, do not belong in this list. This is a fundamental flaw for a paradigm that was explicitly designed not to favour the main languages of the Union. The reasons for this are obvious, only the most "marketable" languages can develop an educational ecosystem strong enough to make these tests economically viable. Sometime, political will can breach the gap like for Spain's regional languages (Galician and Catalan are in the ten languages mentioned above, when Basque only lacks a test for the A1-2 levels), but this will is built on strong institutions and expertise that only a handful of languages have at their disposal in Europe, let alone in the rest of the world. Despite its theoretical grounding, the scarcity of resources (time, money, expertise and interest) make a comprehensive language testing paradigm impractical for most languages, which are once again left behind. Once again, it is the languages that have the most to benefit from these tools, and the most to lose by not using them, that are facing the greatest difficulties to access them. Furthermore, in the case of an adaptive learning system, which is the main motivation for this dissertation, a comprehensive test would be both redundant and unpractical, as the testing would take too much time from the learning experience, unless the testing were to be part of the pedagogy.

We must thus look at more efficient ways to measure proficiency, but before this, we need to develop a deeper understanding of what language acquisition means, how the abstract theoretical knowledge present in the unread dictionaries and grammars articulates with the two or four practical skills that characterize daily language use of all known human cultures around the globe. What is competence and performance in regards to language proficiency?

    \subsection{The Intertwined Nature of the Proficiency Constructs}
Most theories in linguistics, especially de Sausure's structuralism and Chomsky's generativism, are based on an analytic approach, first taking language in isolation from other mental processes, then separating its conceptual constituents, lexis from grammar, competence from performance \parencite{chomsky_aspects_1965} and repeating process with their constituents and subconstituents, to then study the ways to combine them together. In a way, the CEFR paradigm follows the same epidemiologic trend, by dividing production and perception skills, oral and written usages. The main benefit of these analytical methods is obvious, by separating aspects and categories, one can cover an exhaustive understanding of the constituents and rules of a complex systems such as languages. But despite its strength, this analytical approach brings a biased view as to what a language is, as it brings a static and isolated representation to the systems it studies. However, languages, or for this matter language knowledge, never are a fully static structure nor a succession of synchronic states, because languages live the human flesh, they have to be acquired and forgotten by every passing generation and are never stagnant, nor limited to their internal structure. This is where modern approaches, like functionalism or cognitive linguistics \textcite{evans_cognitive_2009} come into play, along by developmental psycholinguistics, by bringing the focus to the acquisition and use of the language and it's relation to the body, rather than its structure. \textcite{bybee_usage-based_1999} argues that usage-based linguistics can produce formal models, but with a twist. By stating that the competence comes as the formalisation of usage, almost as an emerging property, and this usage of the language being primarily a social, physical, embodied and cognitive activity, this new paradigm brings new considerations into light. Where generativism view performance as the materialisation of innate structures of the brain giving the structure precedence over anything linguistic, usage-based approaches consider structures as generalisation made by the language learning brain. This view goes beyond the simple inversion of precedence in what is an obvious chicken-egg situation. By insisting that cognitive processes always have some degree of dependence on embodied, sensorimotor processes, this view also breaks the Cartesian mind-body duality \parencite{varela_embodied_1991} as well as Chomsky's competence-performance duality. In simple words, everything in the brain is (or eventually becomes) connected based on usage, and structures always come a posteriori.

These developments in linguistics proper are also supported by recent advance in neurology. Since their discovery by Vermon Mountcastle in the 1950's, it has been debated whether the cortical columns inuformally structuring the the grey matter in the neocortex play a role as a modular unit of computation \parencite{horton_cortical_2005}. The thousand brains hypothesis \parencite{hawkins_theory_2017, hawkins_thousand_2021} is the latest iteration of this idea and proposes a model on how this unique architecture can, through a voting mechanisms, progressively map sensorimotor inputs towards and from different degrees of abstractions and to refine a unified representation of the world, and thus better engage with it in a continuous feedback loop. This produces a compelling argument on how abstract thinking and language can progressively emerge from sensorimotor interactions \parencite{constantinescu_organizing_2016}, when Chomsky's genes of a Universal Grammar are still waiting to be found anywhere.

        \subsubsection{Implications for Language Testing}
        \abbrv{L1}{First Language or Mother Tongue}
        \abbrv{L2}{Second Language}
At this point, the parallel between the CEFR testing paradigm and formal linguistics has to be clarified, because in the CEFR paradigm, in a way, we measure performance to deduce competence, so the link between those is never denied. But the epistemological critique of the quest exhaustiveness as undermining the understanding of the dynamics of the acquisition process still stands. If we are interested in the acquisition process and its dynamics, a complete, static representation of the skills is counter-productive. Furthermore, if the competence does not exist independently from the performance, could the skills be deduced from knowledge itself? This is what functionalist linguistics seems to argue for.

If everything is connected, if all is one (though one is not all), that is, if more practice leads to better practical skills, or performance, which leads to better theoretical knowledge, or competence, then, performance could in theory be measured through any construct describing competence, such as vocabulary knowledge. Vocabulary is especially interesting as its acquisition is a discrete, yet, never-ending process during a language learning journey. \textcite{eun_hee_jeon_understanding_2022} published a series of meta-analyses on the correlates of the different practical skills defined by the CEFR, all pointing towards this direction, with vocabulary knowledge being cited as a strong correlate for proficiency in listening \parencite{innami_meta-analysis_2022}, speaking \parencite{jeon_meta-analysis_2022}, reading \parencite{jeon_updated_2022} and writing \parencite{kojima_meta-analysis_2022}. Note however that this does not mean that vocabulary knowledge causes fluency, although it contributes to it to the extent that fluency does not come without an advanced level vocabulary knowledge. This basic premise opens the door for low stake, low-cost, LRLs-friendly quick testing which may be more scalable and applicable in many areas, from self-assessment, to the development of automated language learning tracing systems mentioned in the introduction. Notably, in the context of LRLs, that some may call "oral languages", the idea that higher vocabulary level is linked to practical skill becomes even more likely, because the dominant way to access knowledge is a "more integrated usage" (one doesn't learn Rapa Nui in the books). This way, one may even posit that vocabulary testing becomes increasingly relevant as less written and digital resources are available to a given language. 

The last implication of this first-principle and connectionist view of language acquisition is the absence of practical difference between the way competence in the first language (L1) and a second language (L2) are acquired, that is, through usage. Once the circuitry responsible for verbal communication is unlocked between age 1 and 6, either through monolingual (including a sign language) or multilingual education, the way new words are acquired is consistent across the languages spoken by a multilingual. If a word or a feature is discovered through integrated usage and the piece of knowledge in the brain stems from a sensorial experience present during the acquisition of the term, and if a word in L2 is learned as the translation of a word in L1, its representation in the brain will stem from the L1 word as its synonym within another "register" which is the network of the L2. The two scenari implying a formation of knowledge from the usage context but with no difference in status between the L1 and L2 networks. A word can be learned in L2 as the product of an integrated experience, and its L1 equivalent can be learned at a later stage as a "synonym in another space". As someone who learned about back-propagation in English first, my third language, I can assure the reader that I still need to think about the English word before finding its translations during a conversation in French or Breton. Once again, this equivalence between L1 and L2s is convenient in the context of LRLs, because these languages are often the low variety in diglossic regions, where the notion of native speaker and the line between L1 and L2 are often blurred.

    \subsection{A topography of Vocabulary Tests}
It has often been shown that well-chosen proxies can give a reliable understanding of complex processes that one tries to measure. Economists have for example shown how nightlight measurement from space can serve as a reliable growth indicator in countries where official statistics may be lacking in quality or honesty \parencite{henderson_measuring_2009}, even without providing a causal mechanism for why this may work. Linguists imagined many ways to define and measure vocabulary knowledge, as they understood and demonstrated the strong correlation it had with other constituents of language proficiency. This last part of this first section of the literature review will give an overview of the different ways linguists attempted to measure vocabulary so far.

        \subsubsection{Productive Vocabulary Tests}
The most integrated ways to test vocabulary consist of asking the test takers to give a synonym of a word, thus assessing the productive vocabulary skills, the words that the testees can, not only recognize and understand, but also retrieve from its meaning only. It is one of the strategies used to measure the vocabulary index, which is combined with three other indices to calculate the so-called IQ of the test taker in the Wechsler adult and children intelligence scales \parencite{wechsler_wechsler_nodate}.

        \subsubsection{Receptive Vocabulary Tests}
        \abbrv{VLT}{Vocabulary Levels Test}
In the middle are found a series of tests that aim to measure receptive vocabulary skills, the words that can be associated with their meaning by the test takers. The most widely used of those is the Vocabulary Levels Test (VLT), developed in the 1980s by \textcite{nation_teaching_1990} (see \cite{kremmel_vocabulary_2017} for more details on its implementation, evolution and application). This test was designed for widespread use in schools as a placement tests for students. VLT is somewhat adaptive too, as it is testing the skills to associated terms related in meaning from different frequency ranges. An interesting receptive vocabulary test design is the Peabody Picture Vocabulary Test \parencite{dunn_ppvt-4_nodate}. As it is based on pictures instead of written words, it allows testing children who could not otherwise read the words assessed. This picture-based approach could seem to make this testing design an ideal candidate for translation, and thus a candidate for a universal standard that could be applied even in environments where literacy is not widespread. However, this idea may be good only on appearance, as the calibration for the pictures-words mapping took place in an English-speaking country, and the words that may be used to describe similar situations may vary greatly between different linguistic spaces. This is what \textcite{kartushina_use_2022} learned the hard way as they tried to translate the test in Russian for preschoolers, somewhat accidentally demonstrating that the Peabody test may be one of the hardest vocabulary tests to port to other languages, even ones spoken in a somewhat WEIRD society like Russia.

        \subsubsection{Recognition Vocabulary Tests}
\abbrv{LDT}{Lexical Decision Task}
\abbrv{SDT}{Signal Detection Theory}
Finally, the simplest family of vocabulary tests are recognition vocabulary tests, sometime simply called simple vocabulary tests, they measure the aptitude to merely recognize the presence of a word, without requiring the justification of a further understanding of the meaning of the word. For an overview and assessment of different designs, see \cite{meara_complexities_1994}. The most successful design of this vocabulary testing family are the lexical decision task (LDT) vocabulary test, they were given many other names such as "Yes/No" or "binary" vocabulary tests, but all follow the same principle; a sequence of testing items, either real words of a pseudo-words \parencite{meara_imaginary_2012} are presented to the test takers, who is systematically asked whether they think the item belongs to the lexis of language concerned. The results come in a combination of the four output defined by a confusion matrix, hits, misses, false alarm and correct rejection and different methodologies have been proposed to treat the results, from subtracting the percentage of the wrong answers from the percentage of correct answers, up to applying more complicated systems from Signal Detection Theory (SDT) \parencite{huibregtse_scores_2002}.

Many such tests have been built so far include at least one online version, and, encouraging fact, available in several languages English, Dutch and German \parencite{lemhofer_introducing_2012}. This paper showed encouraging results, with strong correlation of the vocabulary result with other traditional tests, thus supporting the idea that proficiency can be effectively measure through vocabulary testing. Another test has apparently been made for Croatian \parencite{srce_how_2025}, although more information is not yet available. And this is in parallel with the numerous systems developed by Meara over the years\cite{meara_complexities_1994}. The main limitation of these systems is the fact that their items are limited and static, so they are never designed for a repeated usage, which would help measure the dynamics of vocabulary acquisition. This is a problem to be fixed, because the main interest of a minimalist test is to allow recurrent testing.

    \subsection{Relevance and Limitations of Vocabulary Tests}
All the vocabulary tests presented above had their load of commercial or academic success due to their reliability in capturing different aspects of vocabulary acquisition. This shared reliability even works against the idea of seeing any of those becoming a standard, because they would all play an equally relevant part in this matter. We already explained the reasons why this should be so in section 2.1.2. If one admits that any sub-construct of proficiency is linked in the brain in a way defined by usage, that "all is one", then the same logic applies to vocabulary. Recognition comes as the first stage of vocabulary acquisition, without which any further development towards a more integrated usage is impossible. All these testing families measure different stage of the same integrated process of vocabulary acquisition. Nightlight measurement does not only measure the "nighttime electric consumption dedicated to street lightning of a territory" construct, but, as the statistics showed, it can be used as a GDP indicator, which is itself an indicator of economic health. The same goes for these vocabulary tests, they all are different constructs measuring the same phenomenon of vocabulary acquisition, which is an integral part of language acquisition.

The main differences between these tests are how resource-intensive they are and how integrated the constructs they measure are. Simple indicators like mere vocabulary recognition have weaknesses and can be subject to cheating or manipulation. The Economist's famous Big Mac index for inflation was allegedly the target of manipulation attempts by the Argentinian government in 2011 \parencite{politi_argentinas_2011} for this very reason. Similarly, the simpler to acquire a construct used as an indicator is, the more likely it is to become subject to manipulation attempts. But this does not mean that the construct as no value, indeed, both nightlight and Big Mac prices levels are still used today, but in scopes and at stakes relevant to their complexity. The same goes for psychometrics. In the context of vocabulary tests, the Peabody picture test's resource intensive design requirement make it need commercial use to support its complex development. The other, simpler tests achieve only academic success because they are so simple to put in place that they never need commercialisation, which limits their scaling potential and in turn their development. Nonetheless, they are all equally useful in measuring their respective stages of vocabulary acquisition.

    %\subsection{Considerations on Construct Validity}

    \subsection{Conclusion}
In the context of an automated and adaptive testing with the purpose of tracing the acquisition of language skills, the vocabulary tests advantages outweigh largely other methods, and among them, the simpler vocabulary recognition tests designs truly shine, especially when considering the problem posed by LRLs. LDT vocabulary tests are simpler to administer in a fully automated way, and they are easier to port to LRLs because they can be derived from a simple list of dictionary entries. Yet, significant challenges remain before enabling a widespread implementation of LDT vocabulary test. The main limiting factor being the number of items proposed in the tests, both real and pseudo-words had to be selected from a larger set during a preliminary study in \cite{lemhofer_introducing_2012}. If an LDT vocabulary test is to be used in a recurrent way, to trace vocabulary progress through time, the items available for testing must be plentiful, maybe cover the whole lexis of a language or at least a significant portion of it. But then the question of the items calibration kicks in. There can be no question of thinking of scaling the preliminary study done for selecting the items in LexTALE to get enough items to allow reliable recurrent testing, already for a language with tremendous resources like English, let alone LRLs. Solving this problem of the items calibration would open the door to scaling both vertically (allow recurrent testing for the same language) and horizontally (allow porting the test to many languages). The next section will be dedicated to finding such a solution.

\section{Knowledge Tracing}
    \abbrv{KT}{Knowledge Tracing}
    \abbrv{CAT}{Computerized Adaptive Testing}
To paraphrase \textcite{meara_complexities_1994}, many assessment tasks may be valid ways to assess vocabulary recognition skills, be provided the appropriate method of analysis. This section is dedicated to this problematic. Measuring latent traits from a tests items responses is a complex task known as Knowledge Tracing (KT) \parencite{shen_survey_2024}, which is a fundamental concept in Computerized Adaptive Testing (CAT). Part of this complexity depends on the assumptions one makes on the latent traits, are they a continuous construct or a set of discrete skills, which combine together in a multidimensional knowledge space, and if so, which skill depends on which others? These dimensions and the relationships between them can be defined manually or based on data, using Bayesian techniques or DL. Other assumption may include the influence of the testing process on the learning process, in which case one may factor in the half-life of new memories formed during previous assessment rounds. Fundamentally, this complicated choice of the model is an arbitration between accuracy and interpretability \parencite{pelanek_adaptive_2025}. More qualitative models may be appropriate to inform recommendations of learning material, but presenting a proficiency vector as the result of a stand-alone test may be impossible to interpret than a single result.\\
Since this dissertation primarily focus on testing, a unidimensional, quantitative index seems more appropriate.  Furthermore, the calibration of a qualitative paradigm would require large amount of data or resources like time and expertise, which are unavailable for LRLs. The end of this chapter will lay down the theoretical basis a this quantitative interpretation of the results of a LDT vocabulary test.

    \subsection{Theoretical Capacity of a Noiseless Unidimensional Test}
The goal of the knowledge tracing model in a CAT is to make predictions on the outcome future test items in order to select the items whose answer are the most uncertain based on previous results. In the information theory jargon, this is called maximizing the entropy, which maximizes the gain of information by the model by minimizing its uncertainty. Drawing from \textcite{shannon_mathematical_1948}, one can define the theoretical absolute capacity of a noiseless binary test, before adapting it to a noisy environment. In a simple, unidimensional scale, finding this spot of highest uncertainty can be achieved with the binary search algorithm. Take a list of items ordered by difficulty, take an item in the middle,  repeat the the process with the second half of the original list if the answer is right, else, with the first half. Repeat the process until the list is one item long. This algorithm has a time complexity of $\theta(\log{n})$, which means that for $n$ number of items, $log_2(n)$ steps are required to reach the last item. This is 10 items need to be tested for a scale containing 1 024 items, 11 for 2 048 items, 12 for 4 096 and so on...

Supposing that all the words in a dictionary of 30 000 words could be ordered by "difficulty", and that half the items of a test have to be pseudo words to deter cheating, a test using this algorithm would find the test taker's current level in only 30 rounds of testing, to compare with the 60 items used by a test like LexTALE \parencite{lemhofer_introducing_2012}. Even if we take into account the need for error corrections, the total number of step required will remain a proportional to this logarithmic progression. This setup has obvious limitations that we will address in the following subsection, but it bring interesting insights concerning the scaling problem of previous tests. Primarily, it is possible to test a really large number of items in a time efficient way, which opens the door to using the whole lexis of a language as testing items, rather than a selected list of words. This possibility in turn opens the door to unique testing experience, where the chances of going twice through the same testing experience are virtually non-existent. This unlocks the vertical scaling problem that was highlighted earlier in this chapter.

    \subsection{The Elo Rating System}
        \subsubsection{Elo rating and Rasch Model}
        \abbrv{IRT}{Item Response Theory}
        \abbrv{HRL}{high-resource language}
The first obvious limitation of the model previously proposed is the calibration of the items. One cannot get the relative difficulty directly from a dictionary, and the order in which words are acquired by learners may vary greatly depending on various factors. Most vocabulary tests go around this problem by grouping the items by frequency ranges \parencite{nation_teaching_1990, meara_complexities_1994, dudley_context-aligned_2024}. However, possessing frequency lists is often a high-resource language's privilege, and most LRLs don't have such resources at their disposal. For this reason, we propose that the difficulty rating of the words items be directly updated based on the results of the test.

In standardised test, this calibration of the items difficulty if achieved by Item Response Theory (IRT), which is a set of models derived from the Rasch model \parencite{rasch_probabilistic_1980}. The maths behind the Rasch model were rediscovered many times, including outside of the psychometric world, like in chess with the Elo rating system \parencite{elo_uscf_1961, elo_rating_1986}. The key equations for these models are presented below.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
            $$P(X_{AB} = 1)=\frac{1}{1+e^{R_b-R_a}}$$
        \captionof{figure}{Rasch formula}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
            $$P(X_{AB} = 1)=\frac{1}{1+10^{\frac{R_b-R_a}{400}}}$$
        \captionof{figure}{Elo rating system}
        \label{Elo}
    \end{minipage}
\end{figure}

In the Elo rating system, $P(X_{AB} = 1)$ is the probability of player A of rating $R_a$ winning by checkmate against a player B of rating $R_b$. In the Rasch model, $P(X_{AB} = 1)$ is the probability of a test taker of rating $R_a$ to successfully answer at a questionnaire item of difficulty rating $R_b$. Since both follow a logarithmic progression, the rating from a "Rasch rating" to an Elo rating is done by multiplying it by $400/ln(10)$ and inverting the nominator and the denominator to go from Elo to Rasch. The difference in the logarithm base and the addition of a spread factor of 400 in chess was meant to increase readability and interpretability, while matching rating systems previously used in the chess world. A 400 difference in Elo rating means a 1:11 vs 10:11 chance of victory, which is more interpretable than a 1 point difference meaning a 1:2.718 versus 1.718:2.718 odds distribution.

In practice, the main difference between the two systems lies more in the update mechanisms. Since IRT was developed for static tests (without real time adaptive features), it relies on more computationally intensive techniques, which are not well suited for the purpose of a CAT. Its simple updating system is why the Elo rating system has been gaining more attention in the AIED community over the years, \cite{pelanek_applications_2016} mentions several successful integrations of this model in adaptive educative setups, although never for stand-alone tests. The same article also present various update mechanisms that take into account different assumptions, such as correction for cheating strategies or short and middle term memory half-life. The update of an Elo rating is given by the following formula.
\begin{equation}
    R_{A}^{\prime}= R_A+K \times{(S-P)}
    \label{Update Elo}
\end{equation}
The actual score (1 or 0) $S$ is subtracted by the prediction $P$ of the outcome based on the score difference given in \ref{Elo} (value between 0 and 1). If an outcome is certain (more than 800 rating difference) and the result follows the prediction, this value will be close to zero and the change in rating will be close to 0. If the opposite happens, the score increases by a value close to $K$, names the K-factor a value akin to the learning rate in the DL world. This value that may vary depending on the implementations of the rating system, but is often around 20 in the chess world. Sometime, an uncertainty function is used to progressively change the rate of update based on the number of updates (cf. equation \ref{uncertainty-function}).

        \subsubsection{Error Correction and Degeneracy}
MCQ use three category of component, queries (the questions), keys (the right answers) and distractors (the wrong answers). Fundamentally, recognition vocabulary tests are a subset of MCQs, with a unique query for the whole test, and the real words as keys and the pseudo words as distractors. It is acknowledged that there may be different reasons why a test taker may select right or wrong answers. The most obvious one is that a test taker recognises the keys and ignores the distractors. But two other course of actions must be taken into consideration.
\begin{enumerate}
    \item The test taker knows the answer but mistakenly selects a wrong answer (e.g. by answering too quickly and noticing the mistake too late).
    \item The test taker does not know the right answer, and answers properly by pure chance.
    \item The item rating does not correspond to its actual difficulty level because the calibration is not over.
\end{enumerate}
It is understood that these effects add noise to the system and that the test should be made more redundant to compensate these effects. It is understood that if an answer is given for a good reason more than half of the time, the rating of the test taker would still converge towards its real value, although more slowly. Even in a setup where more than half the answers are given for wrong reasons, but the distribution of right and wrong answers is balanced, the model would still be able to avoid degeneracy. But in any case, the number of items tested in a test session shouldn't be made as short as theoretically possible, but take these noise into consideration. Once again, the Elo rating system does this seamlessly with an "uncertainty function". \cite{pelanek_applications_2016} proposes the following uncertainty function to update the rate of the ratings update in function of the number of previous answers.
\begin{equation}
    (n)=a/(1 + bn)
    \label{uncertainty-function}
\end{equation}

Where $a$ and $b$ are positive constants and n the number of previously answered items. The resulting number is used as the K value that multiplies the correction of a rating after an answer. Once again, we'll come back on this aspect in the next chapter.

\section{Conclusion}
This literature review introduced ideas from several fields and attempted to organized them in a coherent whole. From a psycholinguistic argument supporting the idea that vocabulary can be used as a proxy for general language proficiency. To proposing a knowledge tracing model that optimizes the information gained by the results of a binary test. In the next chapter, we shall put these pieces together to build a working recognition vocabulary test.

\chapter{Discussion}
This final chapter is divided in 4 sections. First, an account of direct observations that may inform the focus of future research on the test. Second, we discuss the limitations of the test and the question of interpretability of the scores. Third, we elaborate on the direction that future research on the topic would look like. Finally, we present a conclusion gathering all the contributions of the dissertation as well as an informed answer to the research question.

\section{The Tests in Use}
The information presented here are insights from the author on others' attitude and results when passing the test. They are not supported by data, but only direct observations, and as such they may be subject to biases. However they bring light to blind spots that could not have been foreseen when designing the test.

\subsection{Age and the Relationship to Risk}
When looking at people taking the Breton test for the first time, older people seemed to score better than young people. I recall particularly two young people who were scholarized bilingual schools until age 18, and kept using the language to some extent later, whereas at least one elderly person had no formal education in Breton, and never read books in the language, learning the language through causual social interactions only. It is well possible that older people just have a lot of vocabulary, but the surprise was more how low the score of the young people was. Knowing that these young adults were capable of having fluent conversations in Breton, they are expected to know the most frequent words, yet they both scored below 500. To this day, we see two explaination for this trend. Either the vocabulary acquired by a passive exposition to the language in school may be suboptimal. Either the behaviour of younger people when facing unknown word is different. Older people taking the test took a lot of time to pounder each answer. Younger people took the test quickly, and seemed less risk averse, maybe ready to accept meaning where there is none, or maybe unwilling to recognise their ignorance and limitations. Because of how harsh the score was downgraded when recognising non-words, this unforeseen variation in people's relationship to risk might cause variations in the Breton vocabulary test, regardless of absolute fluency and vocabulary level. It is the reason why the pseudo-words ratings were initialized within the 0–2000 range for the other languages.

Unfortunately there is no more to it than these direct comments. But this observation on the relationship to risk seem consistent with research in social science \parencite{wang_does_2023}. Accounting for this behavioural independant variable in the scoring system was envisaged, like by modeling the tendency to recognize distractors and somehow have the final score maping more to the absolute level in vocabulary recognition skills, if this is even possible while accounting for cheating strategies. This idea was put aside however, for two reasons. For one, the test are intended for self assessment, to measure the progression of individuals through time, not to compare level between students. Secondly, as the test is intended for repeated usage, it is expected that test takers will eventually adapt their behaviour to ``make peace with their ignorance'', be it in order to maximise their results.

\subsection{Worth of the Test as a Learning Tool}
As an advanced beginner in Ukrainian, the author is regularly exposed to the language in an immersion setting. The analysis functionality of the test has proven to be a remarkably useful learning tool. A tool that complement the oral exposition to the language with tailored written feedback, fostering generalisation skills. In the lower range of ratings there are comparatively few items to select from. This means that the test takers are likely to run into these few items in any testing session. In this range, idea that the unrecognised real words in a test session are the next most useful words to learn is really strong. The initial aim behind building a test was to build a technological brick that would help optimize later teaching programs. But it turned out that being able to accuratly answer the question ``Where to start with now''? is already a monumental part in any teaching process. The simple LLM-based feedback, although imperfect seems like the unexpectedly most useful aspect of the test so far. But of course, it relies on everything else that has been discussed so far.

\subsection{Ceiling effects}
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/cy-distribution.png}
    \caption{Distribution of the items in the Welsh tests}
    \medskip
    \small
    We can see that the early test takers had answered a lot of items in the high-900 range and around 1800 level, with a gap in the 1000–1300 range. This looks like a strong indicator of a ceiling effect in the higher range of the test and a ``bottleneck'' effect in the transition between the items rated ramdomly and those rated by frequency rank.
\end{figure}\label{fig:cy-distribution}

All these observations on potential ceiling are based upon preliminary observation and not supported by data. Yet, these may be of interrest to orient future research on this specific phenomenon. 

The Welsh test is the one containing the fewest items so far, only 10722 real words, with rating spreading between 0 and 2000. Some of the few people who have taken it reported a rating around the 2000. This seem to be a strong indicator that a test that does not contain enough items will exhibit a ceiling effect around the highest rated items' rating. However, even when the pool the available items seem much larger, like in Ukrainian, the same problem seemed to remain. The explaination for this is simple. The initial assupmtion was that the tests would behave between a Elo rating system and a proportion-based system, with the items in the 1000–2000 range being randomly rated. But when a test taker knows more than half of the items in the 1000–1300 range, they ultimately know most of the items in the 1700-2000 range. In this initial setting, there is no reason for there rating to stabilize somewhere in the middle. This means that such a large random spread of less frequent real words is unjustified. At most the words not included in a frequency list should be initially spread in the 1000–1400 range. Of course, a ceiling effect would be observed for the first few test sessions, but this is unavoidable. On the other hand, the calibration process would happen much faster. If a relatively easy word has to end up somewhere in the 950–1000 range, it will reach this position faster from an initial 1300 or 1400 rating, than from a 1000 or 2000 rating. The same goes in reverse. The reason for proposing the 400 range span, is the interpretation of the Elo ratings. 400 difference in rating means a 90\% chance of success. So test takers recognising most the randomly distributed items in this range are initially granted a rating that means they have around 90\% chances to recognise the easiest items from this range.
Because the ratings are capped to zero, there are little chances that a ceiling effect would appear in the bottom of the scale. Only a few recognised real-words are needed to reach a positive final score.

The problem of the transition from the 0–1000 range (randomly rated within ranges defined on frequency lists) to the 1000+ range (randomly rated within one large range) is tighly linked to the problem mentioned earlier. The easiest items from the upper band must make their way down as fast as possible. To ensure a smoother transition in this critical junction, we propose that the two ranges be spaced from one another. The gap would be filled by the harder items from the frequency lists and the easiest randomly rated items. A gap of 100 points, would not slow down the progression of the test takers ratings.

\section{Limitations and Interpretation of the Scores}
An easy misinterpretation to make about the test score would be to consider all the words below a final score as mastered by the test takers and those rated above the final score as unrecognisable by them. This is not exactly what the reasults imply. The final score is supposed to represent the level at which a student recognise only 50\% of the words, without necessarly understanding their meaning. An item rated 677 points less than the final score has 99\% chances of being rightfully recognised. Conversely, an item rated 677 points more than a final score would be recognised 1\% of the time. As there are more items in the higher ranges, if the final score is in the lower ranges, this 1\% is ``bigger'' than the 1\% of few items in lower ranges. In simples terms, this means that, indeed, almost all words in the lower ranges below a given rating are expected to be well known, (including with a stronger understanding of their meanings). But many words in the higher range could still be recognised.

This test looks for a breaking point in the student knowledge, rather that showing exactly all the words that they know. Comparing again with the CEFR paradigm, which is based on a ``can-do'' approach \parencite{europe_common_2020}, this paradigm looks for the ``can't-do''. In practice it will work the same for a majority of cases, because of the normative nature of the Elo system update mechanism. However, an potential limitation is to be highlighted here.

People are not to be expected to learn languages the same ways. When using a same language as children, partners, students, scholars, tourists, professionals or missionaries, people may need to use master divergent lexical fields. To which extend the vocabulary of these different learners overlaps is an open question. Whether or not this divergence in usage contradicts the statistical interpretation presented above is another question. Eventually, it may become relevant to build clone vocabulary tests in order to target different demographies within the same language. Cloning the test may be the best way to ensure a proper deduction of the ``can-do'' from the ``can't-do'.

\section{Future Research}
We see three different directions of research going forward. The first is about the study of eventual ceiling effects in the tests, and the speed at which the calibration of a test can be considered completed. This aspect is essential to measure the dynamics of language acquisition. The second is about pedagogy, as the test can already be used at least for measuring ``milestone'' levels in vocabulary aquisition. This could be used to experiment on different pedagogical approaches, \textbf{including in diglossic settings}. Finally, the test could be pushed further, including in non-WEIRD environment to study language use divergence and evaluate the need for domain-specific scales.

The two first directions could be researched in parallel with relatively little efforts, by focusing around adult classes of regional languages like Welsh. This would in effect turn the lack of resources and the limited reach of LRLs in a research advantage. The third one would necessitate more resources and most likely international cohordination, with different social groups involved and would likely focus on HRLs.

In the broader field of AIED, the modulo clustering technique appear to be a promising way to build future MCQ tests, whose items would have been mass generated by LLMs. This method could be used to both calibrate their relative difficulty and measure the students level and progress in other fields than L2 acquisition. Especially, new potential research could focus on simulations, to find out what are the optimum parameters (modulo base, spread of the initial items etc...) to build such tests.

\section{Conclusion}
Coming back to the original question, all the element collected in this work cannot disprove the idea that quick adaptive and scalable vocabulary tests can be created for low resource languages. Overall, this vocabulary test design showed that the focus on higher-resource languages in the study of language acquisition is not a fatality. It shows that working with less data can force us into finding orginal solutions. Many challenges have been overcome in this dissertation, although we may end up with more new questions than answers than what we started with. Luckily, these questions are asked along with clear methods for analysis and a refutable hypothesis.

Additionally, it seems to be a good measure to seek to integrate future research on the topic more closely in the usage context of these languages. The initial motivation of this work was the optimization of low resource language teaching. But in this context, can the word optimisation still be understood in the sense of getting more by doing less? Based on the elements gathered in this dissertation, it seems that human skills always grow to fit their usage needs. In this regard, it may well be that the thing that needs the most to be optimised is the time spent using these languages. That is, getting more by doing more, and admitting there is no reason think a technological shortcut exists for a problem that is primarly social. Hopefully, a quick vocabulary test score may become one way to give subtance and increase awareness about the value of this cumulated usage. Not to further optimise this usage, but to maximise it.
